{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exemplo 02: Previsão de feedbacks de produtos B2W**\n",
    "\n",
    "Você recebeu um convite para uma consultoria, na qual deve desenvolver um modelo de previsões de feedbacks de clientes em produtos comprados na loja, que serão coletados do instagram.\n",
    "\n",
    "Os dados que você vai utilizar estão localizados em:\n",
    "https://raw.githubusercontent.com/americanas-tech/b2w-reviews01/refs/heads/main/B2W-Reviews01.csv\n",
    "\n",
    "Na coluna 'review_title' você vai encontrar feedbacks passados dos nossos clientes em nossos produtos e, na coluna 'overall_rating', a nota que foi dada. Esse é o único dado que temos para auxiliar na criação desse modelo de previsões.\n",
    "\n",
    "Dúvidas? Fale comigo!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importando bibliotecas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obtendo dados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtendo dados...\n",
      "                       review_title  overall_rating\n",
      "0                               Bom               4\n",
      "1  Preço imbatível, ótima qualidade               4\n",
      "2      ATENDE TODAS AS EXPECTATIVA.               4\n",
      "3        presente mais que desejado               4\n",
      "4            Sem duvidas, excelente               5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\36131872024.1\\AppData\\Local\\Temp\\ipykernel_25740\\2275861674.py:9: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(ENDERECO_DADOS, sep=',', encoding='utf-8')[['review_title','overall_rating']]\n"
     ]
    }
   ],
   "source": [
    "# Python\n",
    "try:\n",
    "    print('Obtendo dados...')\n",
    "\n",
    "    # constante dos dados\n",
    "    ENDERECO_DADOS = 'https://raw.githubusercontent.com/americanas-tech/b2w-reviews01/refs/heads/main/B2W-Reviews01.csv'\n",
    "\n",
    "    # obtendo dados\n",
    "    df = pd.read_csv(ENDERECO_DADOS, sep=',', encoding='utf-8')[['review_title','overall_rating']]\n",
    "\n",
    "    # excluindo dados não existentes (NaN)\n",
    "    df = df.dropna(subset=['review_title','overall_rating'])\n",
    "\n",
    "    # Tranformando colunas em arrays\n",
    "    texts = np.array(df['review_title'])\n",
    "    rating = np.array(df['overall_rating'])\n",
    "\n",
    "    print(df.head())\n",
    "except Exception as e:\n",
    "    print('Erro ao obter dados: ', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vetorização**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vetorizando texto...\n",
      "[[   0    0    0 ...    0    0    3]\n",
      " [   0    0    0 ... 2620   30   16]\n",
      " [   0    0    0 ...  349   45  155]\n",
      " ...\n",
      " [   0    0    0 ...    0    9    1]\n",
      " [   0    0    0 ...    4   19    3]\n",
      " [   0    0    0 ...    1    4   51]]\n",
      "Textos vetorizados!\n"
     ]
    }
   ],
   "source": [
    "# Biblioteca para trabalhar com redes neurais artificiais\n",
    "# Tensorflow - https://www.tensorflow.org/?hl=pt-br\n",
    "# Tokenizar\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer # type: ignore\n",
    "# ajustar o tamanho do vetor\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences # type: ignore\n",
    "\n",
    "try:\n",
    "    print('Vetorizando texto...')\n",
    "\n",
    "    # Passo 1: tokenizar\n",
    "    tokenizer = Tokenizer()\n",
    "\n",
    "    # Passo 2: Criar o dicionário\n",
    "    # fit_on_texts: Cria o vocabulário, através do dicionário\n",
    "    # associando cada token a um índice numérico\n",
    "    # lembrando que se a palavra aparecer mais de uma vez, ela vai receber o mesmo índice numérico\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "\n",
    "    # Passo 3: Vetorizar, ou seja, transformar os tokens em números,\n",
    "    # a partir do dicionário criado no passo 2\n",
    "    vetores = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    # Passo4: Padronização do tamanho do vetor - pad\n",
    "    padded_vetores = pad_sequences(vetores)\n",
    "\n",
    "    print(padded_vetores)\n",
    "\n",
    "    print('Textos vetorizados!')\n",
    "    \n",
    "except Exception as e:\n",
    "    print('Erro ao vetorizar textos: ', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rede Neural**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construindo a rede neural...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\36131872024.1\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,936,880</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">129,536</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m124\u001b[0m)        │     \u001b[38;5;34m1,936,880\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m129,536\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,066,545</span> (7.88 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,066,545\u001b[0m (7.88 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,066,545</span> (7.88 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,066,545\u001b[0m (7.88 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo configurado e criado\n"
     ]
    }
   ],
   "source": [
    "# Definição do modelo de rede neural utilizada\n",
    "from tensorflow.keras.models import Sequential # type: ignore\n",
    "# Camadas da rede neural\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense # type: ignore\n",
    "# Otimizador de taxa de aprendizado\n",
    "from tensorflow.keras.optimizers import Adam # type: ignore\n",
    "\n",
    "try:\n",
    "    print('Construindo a rede neural...')\n",
    "\n",
    "    # Constantes do modelo\n",
    "\n",
    "    # 1ª Constante: Tamanho do vocabulário\n",
    "    VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "\n",
    "    # 2ª Constante: Tamanho máximo da sequência\n",
    "    # É o comprimento máximo de um texto\n",
    "    MAX_SEQUENCE_LENGHT = padded_vetores.shape[1]\n",
    "\n",
    "    # 3ª Constante: Tamanho do vetor de entrada\n",
    "    # A literatura recomenda que inicia-se por uma quantidade \n",
    "    # igual a raiz quadrada do tamanho do vocabulário\n",
    "    # Se o volume de dados for de larga escala, pode-se testar iniciando em um tamanho maior\n",
    "    # Se o volume de dados for muito pequeno, pode-se testar inciando com um tamanho menor\n",
    "    # Cuidado com o overfitting, que é quando o modelo \"aprende demais\" e, começa a \n",
    "    # perder a capacidade de generalizar melhor, ou seja, obsevar todas as diferenças textuais\n",
    "    # Overfitting pode ser observado no treino da rede neural\n",
    "    VETOR_LENGHT = int(np.sqrt(VOCAB_SIZE))\n",
    "\n",
    "    # Inicia-se a construção da rede neural\n",
    "    # Sequential é um fluxo linear de camandas (conforme visto na Aula02_RNA.pptx)\n",
    "    # São processadas em ordem\n",
    "    model = Sequential()\n",
    "\n",
    "    # Camada de entrada\n",
    "    # Embeddings, na qual os vetores de texto são inseridos\n",
    "    model.add(\n",
    "        Embedding(\n",
    "            input_dim=VOCAB_SIZE,\n",
    "            output_dim=VETOR_LENGHT,\n",
    "            input_length=MAX_SEQUENCE_LENGHT))\n",
    "\n",
    "    # Camada oculta ou intermediária\n",
    "    # LSTM - Long short-term memory, em português \"memória de curto e longo prazo\"\n",
    "    # É onde a magia acontece, ou seja, onde o modelo treina baseado nos seus vetores\n",
    "    # Números de unidades de memória, que basicamente é a qtde de \"neurônios\"\n",
    "    # Primeiro TESTE, EXPERIMENTE somente com 1 camada! Cuidado! com o ovberfitting!\n",
    "    # Se for necessário, adicionar mais camadas, basta repetir o comando abaixo\n",
    "    \n",
    "    # primeirca camada oculta\n",
    "    model.add(LSTM(128))\n",
    "\n",
    "    # Se necessário, adicionar outra camada oculta\n",
    "    # model.add(LSTM(64))\n",
    "\n",
    "    # Camada de saída - Camada Densa\n",
    "    # Regressão, que é o caso desse exemplo. Somente 1 camada\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # construir o modelo\n",
    "    # É literamente pegar as definições anteriores e construir o modelo\n",
    "    # input_shape: é o formato do dados de entrada e ainda o tamanho máximo do texto (MAX_SEQUENCE_LENGHT)\n",
    "    model.build(input_shape=(None,MAX_SEQUENCE_LENGHT))\n",
    "    \n",
    "    # Otimizador de taxa de aprendizado\n",
    "    # importante para ajustar, em casos de overfitting\n",
    "    # Adam: É nosso otimizador que vaia ajustar essa taxa de aprendizado\n",
    "    # learning_rate: Quanto menor, melhor o aprendizado. Menos risco de overfitting\n",
    "    otimizador = Adam(learning_rate=0.0005)\n",
    "\n",
    "    # compilar o modelo\n",
    "    # Verificar se possui algum erro ou se tá de boa\n",
    "    # Além disso, vamos informar o otimizador e a nossa métrica de perda (loss)\n",
    "    # loss - Erro quadrado médio (mean_squared_error)\n",
    "    model.compile(optimizer=otimizador, loss='mean_squared_error')\n",
    "\n",
    "    model.summary()\n",
    "    print('Modelo configurado e criado')\n",
    "\n",
    "except Exception as e:\n",
    "    print('Erro ao construir a rede neural: ', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinar o modelo de rede neural\n",
      "Epoch 1/5\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 65ms/step - loss: 1.8788 - val_loss: 0.6964\n",
      "Epoch 2/5\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 63ms/step - loss: 0.6222 - val_loss: 0.6617\n",
      "Epoch 3/5\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 62ms/step - loss: 0.5393 - val_loss: 0.6590\n",
      "Epoch 4/5\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 62ms/step - loss: 0.4970 - val_loss: 0.6512\n",
      "Epoch 5/5\n",
      "\u001b[1m826/826\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 62ms/step - loss: 0.4691 - val_loss: 0.6681\n"
     ]
    }
   ],
   "source": [
    "# EXPLICAR MELHOR NA PRÓXIMA AULA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "try:\n",
    "    print('Treinar o modelo de rede neural')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        padded_vetores,\n",
    "        rating,\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # o treino da rede neural\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=5,\n",
    "        batch_size=128,\n",
    "        validation_data=(X_test, y_test)\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print('Erro ao treinar a rede neural: ', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step\n",
      "Previsões:  [[ 3.602843  ]\n",
      " [-0.04305703]]\n"
     ]
    }
   ],
   "source": [
    "novos_textos = [\n",
    "    \"Muito bom, gostei bastante. Top demais! Compensa muito!\",\n",
    "    \"Não recomendo, péssimo produto. Não funciona\"\n",
    "]\n",
    "\n",
    "novas_sequencias = tokenizer.texts_to_sequences(novos_textos)\n",
    "novas_sequencias_padded = pad_sequences(novas_sequencias)\n",
    "\n",
    "predicoes = model.predict(novas_sequencias_padded)\n",
    "\n",
    "print(\"Previsões: \", predicoes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
